import gradio as gr
import asyncio
import sys
import wave
import threading
from utils_spe_rec import *
from utils_cam import *
from utils_robot import *
from utils_vlm import *
from utils_vlm_move import *
from utils_tts import *
from utils_micro_bit import *
from start import command_cleaning
from pydub import AudioSegment
from utils_qwen_agent import bot, qwen_agent
def convert_to_wav16k1c(src_path, dst_path):
    audio = AudioSegment.from_file(src_path)
    audio = audio.set_frame_rate(16000).set_channels(1)
    audio.export(dst_path, format="wav")

def check_wav_info(audio_path):
    try:
        with wave.open(audio_path, 'rb') as wf:
            print("channels:", wf.getnchannels())
            print("framerate:", wf.getframerate())
            print("sampwidth:", wf.getsampwidth())
    except Exception as e:
        print("éŸ³é¢‘æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼š", e)
        
def play_tts_in_background(text):
    asyncio.run(text_to_speech(text))
def process_input(text, audio, image, chatbot_state):
    if chatbot_state is None:
        chatbot_state = []

    # å¤„ç†è¯­éŸ³è¾“å…¥
    if audio is not None:
        tmp_path = "/tmp/converted.wav"
        convert_to_wav16k1c(audio, tmp_path)
        print("è¯­éŸ³è¯†åˆ«ç»“æœï¼š", text)
        if text is not None and text != "":
            text = text + "," + recognize_speech(tmp_path)
        else:
            text = recognize_speech(tmp_path)
        # text = text + "," + recognize_speech(tmp_path)

    # å¤„ç†å›¾åƒè¾“å…¥
    if image is not None:
        img_desc = QwenVL_api(text, image)
        text = img_desc if not text else text + ", " + img_desc

    if not text:
        response = "è¯·æä¾›è¯­éŸ³ã€æ–‡æœ¬æˆ–å›¾ç‰‡è¾“å…¥ã€‚"
        chatbot_state.append((text, response))
        return chatbot_state, chatbot_state

    final_response = qwen_agent(text)

    threading.Thread(target=play_tts_in_background, args=(final_response,)).start()
    chatbot_state.append((text, final_response))
    yield chatbot_state, chatbot_state
    
# ä½¿ç”¨ç±»ä¼¼ qwen_agent.gui çš„ç•Œé¢é£æ ¼
with gr.Blocks(theme=gr.themes.Default()) as demo:
    gr.Markdown(
        """
        <div align="center" style="font-size:2.5em; font-weight:bold; color:#333;">
            ğŸ¤– MIRAï¼šå¤šæ¨¡æ€æ™ºèƒ½æœºæ¢°è‡‚åŠ©æ‰‹
        </div>
        <div align="center" style="color:#555;">
            æ”¯æŒæ–‡æœ¬ã€è¯­éŸ³ã€å›¾ç‰‡è¾“å…¥ï¼Œä½“éªŒå¤šæ¨¡æ€AIäº¤äº’
        </div>
        """
    )
    
    with gr.Row(equal_height=False):
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=500, bubble_full_width=False, show_copy_button=True)
            state = gr.State([])
        
        with gr.Column(scale=2, min_width=400):
            txt = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¯·è¾“å…¥æŒ‡ä»¤æˆ–é—®é¢˜...",
                lines=3,
                max_lines=6,
                show_label=True
            )
            
            gr.Markdown("#### è¯­éŸ³è¾“å…¥")
            audio = gr.Audio(type="filepath", label="ä¸Šä¼ è¯­éŸ³æ–‡ä»¶", show_label=True)
            
            gr.Markdown("#### å›¾ç‰‡è¾“å…¥")
            image = gr.Image(type="filepath", label="ä¸Šä¼ å›¾ç‰‡", height=200)

            submit_btn = gr.Button("ğŸš€ æäº¤", variant="primary", size="lg")

    submit_btn.click(
        fn=process_input,
        inputs=[txt, audio, image, state],
        outputs=[chatbot, state]
    )


demo.launch()